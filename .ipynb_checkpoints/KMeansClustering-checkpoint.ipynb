{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Dharmang\n",
      "[nltk_data]     Solanki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "nltk.download('words')\n",
    "from gensim.summarization.summarizer import summarize \n",
    "from gensim.summarization import keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title', 'publication', 'author', 'date', 'year', 'month', 'url',\n",
      "       'content'],\n",
      "      dtype='object')\n",
      "(142570, 9)\n"
     ]
    }
   ],
   "source": [
    "# Load the data into pandas frame\n",
    "data_frame = pd.DataFrame()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for i in range(1,4,1):\n",
    "    try:\n",
    "        path = './data/articles'+str(i)+'.csv'\n",
    "        if os.path.exists(path):\n",
    "            chunk_list = []\n",
    "            reader_obj = pd.read_csv(path,chunksize=10000) \n",
    "            for chunk in reader_obj:\n",
    "                chunk_list.append(chunk)    \n",
    "            data_frame = pd.concat([data_frame,pd.concat(chunk_list).drop(['Unnamed: 0'],axis=1)], ignore_index=True)\n",
    "    except:\n",
    "        # handle the file not found error\n",
    "        print(sys.exc_info())\n",
    "    \n",
    "print(data_frame.columns)\n",
    "print(data_frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a simple text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5607\n"
     ]
    }
   ],
   "source": [
    "# Displaying a sample content\n",
    "sample_content = data_frame['content'][0]\n",
    "print(len(sample_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "### Tokenizations is the process of separating each and every small letter of the sentence.\n",
    "\n",
    "# Removal of Stop Words: \n",
    "### In this process we are also eliminating the stop words in order to extract only words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880\n"
     ]
    }
   ],
   "source": [
    "def tokenization(content):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    token_list = tokenizer.tokenize(content.lower())\n",
    "    return token_list\n",
    "token_list = tokenization(sample_content)\n",
    "print(len(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "def stop_words_filter(token_list):\n",
    "    stopword_set = set(stopwords.words('english'))    \n",
    "    filtered_tokens = []\n",
    "    for token in token_list:\n",
    "        if (token not in stopword_set \n",
    "        and token.isnumeric() == False \n",
    "        and wordnet.synsets(token) != [] \n",
    "        and token.isalpha()\n",
    "        and len(token)>1):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    return filtered_tokens\n",
    "\n",
    "print(len(token_list))\n",
    "filtered_tokens = stop_words_filter(token_list)\n",
    "print(len(filtered_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see we were able to remove a lot of unnecessary words from the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "#### For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "#### The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
    "\n",
    "#### &emsp;  &emsp; am, are, is $\\Rightarrow$ be\n",
    "#### &emsp;  &emsp; car, cars, car's, cars' $\\Rightarrow$ car\n",
    "#### The result of this mapping of text will be something like:\n",
    "#### &emsp;  &emsp; the boy's cars are different colors $\\Rightarrow$\n",
    "#### &emsp;  &emsp; the boy car be differ color\n",
    "#### However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.\n",
    "\n",
    "#### For more information refer: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: In the below method if you uncomment the two print lines you will be able to see what this function is doing. This will help to improve the performance of count-based clustering techniques. Also it will reduce the size of sparse matrix.\n",
    "\n",
    "#### Example there are many examples which are being lemmatized like \n",
    "##### eg 1 . rounds => round \n",
    "##### eg 2 . leases => lease\n",
    "##### eg 3 . jobs => job\n",
    "##### eg 4 . appointees => appointee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization \n",
    "def lemmatize_tokens(filtered_tokens):\n",
    "    lemmatized = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token in filtered_tokens:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token))\n",
    "        #print(\"token: \" + token)\n",
    "        #print(\"Lemmantized \"+lemmatizer.lemmatize(token))\n",
    "    return lemmatized\n",
    "lemmatized = lemmatize_tokens(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(data):\n",
    "    summ = summarize(data, ratio = 0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', 'congressional', 'republican', 'new', 'fear', 'health', 'care', 'lawsuit', 'administration', 'trump', 'administration', 'branch', 'suit', 'challenge', 'administration', 'authority', 'dollar', 'health', 'insurance', 'subsidy', 'american', 'handing', 'house', 'republican', 'victory', 'issue', 'loss', 'subsidy', 'health', 'care', 'program', 'implode', 'people', 'health', 'insurance', 'republican', 'replacement', 'lead', 'chaos', 'insurance', 'market', 'backlash', 'republican', 'control', 'government', 'stave', 'outcome', 'republican', 'position', 'sum', 'health', 'care', 'law', 'voter', 'end', 'law', 'year', 'twist', 'trump', 'administration', 'branch', 'prerogative', 'choose', 'republican', 'ally', 'house', 'central', 'question', 'dispute', 'eager', 'avoid', 'pileup', 'republican', 'capitol', 'hill', 'trump', 'transition', 'team', 'lawsuit', 'election', 'put', 'limbo', 'february', 'united', 'state', 'court', 'appeal', 'district', 'columbia', 'circuit', 'divulge', 'strategy', 'litigation', 'administration', 'congress', 'inappropriate', 'comment', 'spokesman', 'trump', 'transition', 'effort', 'office', 'trump', 'administration', 'case', 'aspect', 'care', 'act', 'decision', 'judge', 'rosemary', 'house', 'republican', 'standing', 'sue', 'branch', 'spending', 'dispute', 'administration', 'health', 'insurance', 'subsidy', 'violation', 'constitution', 'approval', 'congress', 'justice', 'department', 'judge', 'decision', 'subsidy', 'place', 'appeal', 'halt', 'mr', 'trump', 'house', 'republican', 'month', 'court', 'transition', 'team', 'option', 'resolution', 'matter', 'effect', 'inauguration', 'jan', 'suspension', 'case', 'house', 'lawyer', 'administration', 'time', 'appeal', 'leadership', 'official', 'house', 'acknowledge', 'possibility', 'effect', 'payment', 'insurer', 'subsidy', 'exchange', 'cost', 'consumer', 'race', 'drop', 'coverage', 'money', 'loss', 'subsidy', 'destabilize', 'program', 'cause', 'lack', 'confidence', 'lead', 'insurer', 'exit', 'administration', 'mount', 'fight', 'house', 'republican', 'dim', 'view', 'health', 'care', 'law', 'team', 'lawyer', 'month', 'case', 'behalf', 'health', 'care', 'program', 'request', 'lawyer', 'deal', 'house', 'republican', 'new', 'administration', 'dismiss', 'settle', 'case', 'produce', 'consequence', 'individual', 'reduction', 'well', 'nation', 'health', 'insurance', 'health', 'care', 'system', 'house', 'republican', 'concept', 'power', 'purse', 'right', 'congress', 'sue', 'branch', 'constitution', 'power', 'house', 'republican', 'congress', 'money', 'subsidy', 'constitution', 'suit', 'john', 'house', 'speaker', 'time', 'house', 'committee', 'report', 'republican', 'asserted', 'administration', 'funding', 'treasury', 'department', 'skepticism', 'white', 'house', 'part', 'law', 'appropriation', 'administration', 'house', 'judge', 'congress', 'sue', 'white', 'house', 'issue', 'expert', 'precedent', 'leverage', 'branch', 'spending', 'power', 'trump', 'administration', 'pressure', 'presidential', 'authority', 'fight', 'house', 'matter', 'view', 'health', 'care', 'precedent', 'repercussion', 'victory', 'house', 'trump', 'era', 'republican', 'house']\n"
     ]
    }
   ],
   "source": [
    "def find_nouns(content,isTokens=True):\n",
    "    if isTokens:\n",
    "        doc = nlp(' '.join(content))\n",
    "    else:\n",
    "        doc = nlp(content)\n",
    "    tokens = []\n",
    "    for index in range(len(doc)):\n",
    "        if doc[index].pos_ == 'NOUN' or doc[index].pos_ == 'PROPN':\n",
    "            tokens.append(doc[index].text)\n",
    "    return tokens\n",
    "nouns = find_nouns(lemmatized)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-96422b9efbda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-96422b9efbda>\u001b[0m in \u001b[0;36mdata_preprocessing\u001b[1;34m(data_frame)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Step 3: Tokenize the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mtmp_data_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_data_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtokenization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Step 4: Remove stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dharmang solanki\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-96422b9efbda>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Step 3: Tokenize the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mtmp_data_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_data_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtokenization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Step 4: Remove stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d33fc423f03e>\u001b[0m in \u001b[0;36mtokenization\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtoken_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtoken_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtoken_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Create a data pipeline to process the dataset \n",
    "\n",
    "def data_preprocessing(data_frame):\n",
    "\n",
    "    res = pd.DataFrame(columns=['id','content'])\n",
    "    '''\n",
    "\n",
    "    # Step 1: Tokenization\n",
    "    tmp_data_frame = data_frame['content'][0:1000].apply(lambda row:tokenization(row))\n",
    "\n",
    "    # Step 2: Remove stop words\n",
    "    tmp_data_frame = tmp_data_frame.apply(lambda row: stop_words_filter(row))\n",
    "\n",
    "    # Step 3: Make a string\n",
    "    tmp_data_frame = tmp_data_frame.apply(lambda row: ' '.join(row))\n",
    "\n",
    "    res = pd.concat([res,tmp_data_frame])\n",
    "\n",
    "    '''\n",
    "    total_records = len(data_frame)  #50000\n",
    "    start = 0 \n",
    "    interval = ((total_records - start) // 50 ) #1000\n",
    "    # The processing in chunks will reduce the memory load\n",
    "    for i in range(start,total_records,interval):\n",
    "        print(i)\n",
    "        # Step 1: Shorten the text\n",
    "        if(i+interval < total_records):\n",
    "            tmp_data_frame = data_frame['content'][i:i+interval].apply(lambda row:row[:150])\n",
    "        else:\n",
    "            tmp_data_frame = data_frame['content'][i:total_records].apply(lambda row:row[:150])\n",
    "        \n",
    "        # Step 2: find Nouns and Pronouns\n",
    "        tmp_data_frame = tmp_data_frame.apply(lambda row:find_nouns(row))\n",
    "        \n",
    "        # Step 3: Tokenize the text\n",
    "        tmp_data_frame = tmp_data_frame.apply(lambda row:tokenization(row))\n",
    "        \n",
    "        # Step 4: Remove stop words\n",
    "        tmp_data_frame = tmp_data_frame.apply(lambda row: stop_words_filter(row))\n",
    "        \n",
    "        # Step 5: Lemmitize tokens\n",
    "        tmp_data_frame = tmp_data_frame.apply(lambda row: lemmatize_tokens(row))\n",
    "                \n",
    "        # Step 6: Make a string\n",
    "        tmp_data_frame = tmp_data_frame.apply(lambda row: ' '.join(row))\n",
    "\n",
    "        #print(tmp_data_frame.to_frame())\n",
    "        res = pd.concat([res,tmp_data_frame.to_frame()])\n",
    "        \n",
    "    res.columns = ['doc_id','content']\n",
    "    res['doc_id'] = range(0,total_records)\n",
    "    return res\n",
    "\n",
    "df = data_preprocessing(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('intermediate_dataframe.csv', sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('intermediate_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142570, 32029)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "sparse_mat = vectorizer.fit_transform(df['content'].apply(lambda x: np.str_(x)))\n",
    "bag_of_words = sparse_mat\n",
    "word_features = vectorizer.get_feature_names()\n",
    "print(sparse_mat.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31147)\t1\n",
      "  (0, 5747)\t1\n",
      "  (0, 23463)\t1\n",
      "  (0, 18771)\t1\n",
      "  (0, 10446)\t1\n",
      "  (0, 5350)\t1\n",
      "  (0, 12953)\t1\n",
      "  (0, 4118)\t1\n",
      "  (0, 15951)\t1\n",
      "  (0, 363)\t1\n",
      "  (0, 17679)\t1\n",
      "  (1, 3647)\t1\n",
      "  (1, 25363)\t1\n",
      "  (1, 11845)\t1\n",
      "  (1, 6236)\t1\n",
      "  (1, 2922)\t1\n",
      "  (1, 8681)\t1\n",
      "  (1, 30971)\t1\n",
      "  (1, 3990)\t1\n",
      "  (1, 3734)\t1\n",
      "  (1, 20469)\t1\n",
      "  (1, 20393)\t1\n",
      "  (1, 31545)\t1\n",
      "  (1, 24998)\t1\n",
      "  (1, 6482)\t1\n",
      "  :\t:\n",
      "  (142568, 12150)\t1\n",
      "  (142568, 16451)\t1\n",
      "  (142568, 30466)\t1\n",
      "  (142568, 17629)\t1\n",
      "  (142568, 30372)\t1\n",
      "  (142568, 10484)\t1\n",
      "  (142568, 2151)\t1\n",
      "  (142568, 21847)\t1\n",
      "  (142568, 14071)\t1\n",
      "  (142568, 23121)\t1\n",
      "  (142569, 19406)\t1\n",
      "  (142569, 10745)\t1\n",
      "  (142569, 31887)\t1\n",
      "  (142569, 584)\t1\n",
      "  (142569, 3025)\t1\n",
      "  (142569, 28665)\t1\n",
      "  (142569, 18571)\t1\n",
      "  (142569, 18631)\t1\n",
      "  (142569, 17088)\t1\n",
      "  (142569, 23504)\t1\n",
      "  (142569, 12423)\t1\n",
      "  (142569, 5310)\t1\n",
      "  (142569, 25464)\t1\n",
      "  (142569, 25524)\t1\n",
      "  (142569, 22893)\t1\n"
     ]
    }
   ],
   "source": [
    "print(sparse_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "vectorizer = TfidfVectorizer()\n",
    "sparse_mat = vectorizer.fit_transform(df['content'].apply(lambda x: np.str_(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17679)\t0.31733068766883377\n",
      "  (0, 363)\t0.2861069135025976\n",
      "  (0, 15951)\t0.35504476391642836\n",
      "  (0, 4118)\t0.3229931454598027\n",
      "  (0, 12953)\t0.3048624260763353\n",
      "  (0, 5350)\t0.2944498605403032\n",
      "  (0, 10446)\t0.3790106499719746\n",
      "  (0, 18771)\t0.19166000289408724\n",
      "  (0, 23463)\t0.21743586085268013\n",
      "  (0, 5747)\t0.3540623685833741\n",
      "  (0, 31147)\t0.23323753808648456\n",
      "  (1, 5239)\t0.22289481572704153\n",
      "  (1, 12086)\t0.21367200037936315\n",
      "  (1, 24681)\t0.20803979449563142\n",
      "  (1, 6482)\t0.19778754162123005\n",
      "  (1, 24998)\t0.17446285866402936\n",
      "  (1, 31545)\t0.23510451101650157\n",
      "  (1, 20393)\t0.2672835677747418\n",
      "  (1, 20469)\t0.12850395121404842\n",
      "  (1, 3734)\t0.24544329635226422\n",
      "  (1, 3990)\t0.30173824393281884\n",
      "  (1, 30971)\t0.3701943686846686\n",
      "  (1, 8681)\t0.2723164458119942\n",
      "  (1, 2922)\t0.2312460517975244\n",
      "  (1, 6236)\t0.27835084003259675\n",
      "  :\t:\n",
      "  (142568, 2151)\t0.3133381152352469\n",
      "  (142568, 10484)\t0.32206390997016526\n",
      "  (142568, 30372)\t0.2353047940668551\n",
      "  (142568, 17629)\t0.26268120776376275\n",
      "  (142568, 30466)\t0.2281840039080983\n",
      "  (142568, 16451)\t0.24176443499180233\n",
      "  (142568, 12150)\t0.19434169163755285\n",
      "  (142568, 27248)\t0.1856491239915843\n",
      "  (142568, 15940)\t0.20913213802387864\n",
      "  (142568, 18771)\t0.13278947675561903\n",
      "  (142569, 22893)\t0.36383949485651534\n",
      "  (142569, 25524)\t0.3206227919159021\n",
      "  (142569, 25464)\t0.3042186542638573\n",
      "  (142569, 5310)\t0.28785368605816114\n",
      "  (142569, 12423)\t0.25356564048019964\n",
      "  (142569, 23504)\t0.2687881815904432\n",
      "  (142569, 17088)\t0.3042186542638573\n",
      "  (142569, 18631)\t0.22127107974264795\n",
      "  (142569, 18571)\t0.2132548988928694\n",
      "  (142569, 28665)\t0.19927884311989677\n",
      "  (142569, 3025)\t0.25307776384179087\n",
      "  (142569, 584)\t0.20904740300468097\n",
      "  (142569, 31887)\t0.15013362984762987\n",
      "  (142569, 10745)\t0.16278632909917393\n",
      "  (142569, 19406)\t0.26340507959380705\n"
     ]
    }
   ],
   "source": [
    "print(sparse_mat)\n",
    "word_features = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=200)\n",
    "sparse_mat = svd.fit_transform(sparse_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "sse = []\n",
    "davies_score =[]\n",
    "'''\n",
    "list_k = list(range(2, 20))\n",
    "for k in list_k:\n",
    "    kmeans = KMeans(n_clusters=k, random_state = 0).fit(sparse_mat)\n",
    "    l = kmeans.labels_\n",
    "    result = metrics.davies_bouldin_score(sparse_mat, l)\n",
    "    davies_score.append(result)\n",
    "    sse.append(kmeans.inertia_)\n",
    "'''\n",
    "kmeans = KMeans(20,init='k-means++', max_iter=100, n_init=1,random_state=0).fit(sparse_mat)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.989149438856667\n"
     ]
    }
   ],
   "source": [
    "result = metrics.davies_bouldin_score(sparse_mat,labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'kmeans.pickle'\n",
    "pickle.dump(kmeans, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.989149438856667\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "kmeans = pickle.load(open(filename, 'rb'))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "[[17  1 14 ...  3 51 13]\n",
      " [42 41 92 ... 44 78 45]\n",
      " [ 1  2 19 ...  6  3  4]\n",
      " ...\n",
      " [ 2 13 15 ... 20 10 19]\n",
      " [ 3  1  0 ...  2  4  5]\n",
      " [ 1 21 13 ... 19  4 15]]\n",
      "Cluster 0:\n",
      " abc\n",
      " aaa\n",
      " abbey\n",
      " abbot\n",
      " abortion\n",
      " abatement\n",
      " abandon\n",
      " abolition\n",
      " abortifacient\n",
      " aa\n",
      "Cluster 1:\n",
      " abomination\n",
      " abolitionist\n",
      " accentuate\n",
      " aaa\n",
      " abysmal\n",
      " acceleration\n",
      " abound\n",
      " abyss\n",
      " academic\n",
      " accompany\n",
      "Cluster 2:\n",
      " aaa\n",
      " aahs\n",
      " abdication\n",
      " aa\n",
      " abbe\n",
      " abstruse\n",
      " ab\n",
      " abode\n",
      " abhorrent\n",
      " abdomen\n",
      "Cluster 3:\n",
      " aahs\n",
      " aaa\n",
      " aarp\n",
      " aaron\n",
      " abate\n",
      " abandonment\n",
      " abdication\n",
      " aa\n",
      " abdomen\n",
      " aba\n",
      "Cluster 4:\n",
      " aboriginal\n",
      " abnormality\n",
      " aaa\n",
      " absurdity\n",
      " ability\n",
      " acapulco\n",
      " abortifacient\n",
      " acacia\n",
      " abortive\n",
      " abomination\n",
      "Cluster 5:\n",
      " aa\n",
      " aarp\n",
      " abandonment\n",
      " aaron\n",
      " abate\n",
      " actin\n",
      " adage\n",
      " acrimonious\n",
      " abet\n",
      " acclaim\n",
      "Cluster 6:\n",
      " accustomed\n",
      " accumulation\n",
      " aaa\n",
      " accreditation\n",
      " accent\n",
      " acquirer\n",
      " accountability\n",
      " accra\n",
      " acapulco\n",
      " accessory\n",
      "Cluster 7:\n",
      " abound\n",
      " aaa\n",
      " abscess\n",
      " abortionist\n",
      " abaya\n",
      " abdication\n",
      " abhors\n",
      " abortifacient\n",
      " absolute\n",
      " absence\n",
      "Cluster 8:\n",
      " aaa\n",
      " academe\n",
      " abrasive\n",
      " abet\n",
      " abaya\n",
      " abyssinian\n",
      " absent\n",
      " absolution\n",
      " abatement\n",
      " aby\n",
      "Cluster 9:\n",
      " aaa\n",
      " ab\n",
      " aaron\n",
      " aarp\n",
      " abandonment\n",
      " abbreviation\n",
      " aa\n",
      " abbe\n",
      " abbey\n",
      " abdicate\n",
      "Cluster 10:\n",
      " aback\n",
      " abandon\n",
      " aaa\n",
      " ab\n",
      " abysmal\n",
      " abundance\n",
      " academe\n",
      " abyss\n",
      " abuse\n",
      " abandonment\n",
      "Cluster 11:\n",
      " abatement\n",
      " abaya\n",
      " aaa\n",
      " aberdeen\n",
      " abduction\n",
      " abdicate\n",
      " abandonment\n",
      " abound\n",
      " abhors\n",
      " abnormality\n",
      "Cluster 12:\n",
      " aaa\n",
      " aarp\n",
      " aba\n",
      " abandon\n",
      " abate\n",
      " aa\n",
      " abandonment\n",
      " abbe\n",
      " abdication\n",
      " abel\n",
      "Cluster 13:\n",
      " abomination\n",
      " abolitionist\n",
      " aaa\n",
      " abyssinian\n",
      " abound\n",
      " accelerator\n",
      " abode\n",
      " accession\n",
      " acadia\n",
      " abstention\n",
      "Cluster 14:\n",
      " abuser\n",
      " aby\n",
      " absurdity\n",
      " abysmal\n",
      " abstruse\n",
      " academia\n",
      " acacia\n",
      " abuse\n",
      " absorb\n",
      " absolve\n",
      "Cluster 15:\n",
      " aback\n",
      " abandon\n",
      " aaa\n",
      " abstinence\n",
      " aby\n",
      " ab\n",
      " abortion\n",
      " absolve\n",
      " abscess\n",
      " absolute\n",
      "Cluster 16:\n",
      " aaa\n",
      " aback\n",
      " aarp\n",
      " abdicate\n",
      " aa\n",
      " abduction\n",
      " abbot\n",
      " abhorrent\n",
      " absorption\n",
      " abc\n",
      "Cluster 17:\n",
      " aahs\n",
      " abbe\n",
      " abbot\n",
      " abaya\n",
      " abdicate\n",
      " abbey\n",
      " aback\n",
      " abandon\n",
      " abc\n",
      " aaa\n",
      "Cluster 18:\n",
      " aaron\n",
      " aaa\n",
      " aa\n",
      " abaya\n",
      " aba\n",
      " aback\n",
      " abbot\n",
      " abdication\n",
      " abduct\n",
      " abate\n",
      "Cluster 19:\n",
      " aaa\n",
      " abduct\n",
      " abbe\n",
      " abc\n",
      " abet\n",
      " abbreviation\n",
      " abandon\n",
      " abduction\n",
      " aahs\n",
      " abel\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "nouns = find_nouns(terms)\n",
    "\n",
    "print(order_centroids)\n",
    "\n",
    "for i in range(20):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :50]:\n",
    "        if(terms[])\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.07344369e-02  1.57601380e-01 -1.56058372e-02 ...  3.21213624e-02\n",
      "  -3.73841500e-02 -2.13398307e-02]\n",
      " [ 2.15175786e-03  2.29269456e-02  1.12625651e-02 ...  2.17262805e-03\n",
      "   1.15333696e-02  3.26789865e-03]\n",
      " [ 8.61380978e-04  1.08273697e-02  1.63818347e-03 ... -5.60145157e-05\n",
      "   1.68493318e-03 -5.63649570e-03]\n",
      " ...\n",
      " [ 3.12336727e-03  4.34743464e-02  2.16035155e-03 ...  6.38126359e-02\n",
      "   4.27860801e-03  3.11875877e-02]\n",
      " [ 4.59887526e-02  4.88799498e-02  1.43258030e-02 ... -3.59895575e-03\n",
      "  -4.34719733e-02 -3.14120768e-02]\n",
      " [ 4.60258284e-03  6.18189004e-02  2.19000390e-02 ...  7.86091462e-03\n",
      "  -4.21572858e-02 -4.21243906e-03]]\n",
      "32029\n"
     ]
    }
   ],
   "source": [
    "print(sparse_mat)\n",
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(6, 6))\\nplt.plot(list_k, sse, '-o')\\nplt.xlabel(r'Number of clusters *k*')\\nplt.ylabel('Sum of squared distance');\\nplt.savefig('results/20ClustersSSE.png')\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot sse against k\n",
    "'''\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');\n",
    "plt.savefig('results/20ClustersSSE.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(6, 6))\\nplt.plot(list_k, davies_score, '-o')\\nplt.xlabel(r'Number of clusters *k*')\\nplt.ylabel('Davies Scores');\\nplt.savefig('results/DaviesScore_20Clusters.png')\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, davies_score, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Davies Scores');\n",
    "plt.savefig('results/DaviesScore_20Clusters.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Optimal Cluster size: \"+ str(list_k[np.argmin(davies_score)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the topic name\n",
    "clusters = []\n",
    "for i in range(0,20,1):\n",
    "    cluster_tuple = np.where(labels == i)\n",
    "    clusters.append(cluster_tuple[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    17     31    198 ... 142440 142477 142562]\n"
     ]
    }
   ],
   "source": [
    "print(clusters[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = bag_of_words.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 3 2 ... 2 8 1]]\n"
     ]
    }
   ],
   "source": [
    "print(sum_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec = CountVectorizer().fit(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_freq = {}\n",
    "#for word, idx in vec.vocabulary_.items():\n",
    "#    word_freq[word] = sum_words[0, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_freq = {k: v for k, v in sorted(word_freq.items(), key=lambda item: item[1],reverse=True)}\n",
    "#word_list = list(word_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     5,     20,     82, ..., 142508, 142543, 142544], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['trump', 'washington', 'president', 'year', 'state', 'new', 'week', 'day', 'united', 'tuesday', 'time', 'monday', 'thursday', 'wednesday', 'friday', 'city', 'email', 'people', 'morning', 'york']\n",
      "1\n",
      "['trump', 'president', 'washington', 'year', 'state', 'new', 'united', 'day', 'time', 'week', 'tuesday', 'morning', 'people', 'wednesday', 'friday', 'city', 'monday', 'house', 'sunday', 'york']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Try to find the proper nouns \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from collections import Counter\n",
    "import nltk\n",
    "tags = []\n",
    "for i in range(len(clusters)): \n",
    "    print(i)\n",
    "    all_nouns = []\n",
    "    # for each document in the cluster\n",
    "    for doc_id in range(len(clusters[i])):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(str(df['content'][doc_id]))\n",
    "        #tokens = tokenization(df['content'][doc_id])\n",
    "        # select first 50 and last 50 words from preprocessed data\n",
    "        # This is the assumption that first 200 characters of news articles will cover the \n",
    "        # abstract idea of the complete passage\n",
    "        \n",
    "        doc = nlp(' '.join(tokens))\n",
    "        nouns = []\n",
    "        for index in range(len(doc)):\n",
    "            if doc[index].pos_ == 'NOUN' or doc[index].pos_ == 'PROPN':\n",
    "                nouns.append(doc[index].text)\n",
    "        nouns = Counter(nouns) \n",
    "        nouns = {k: v for k, v in sorted(nouns.items(), key=lambda item: item[1],reverse=True)}\n",
    "        i = 0\n",
    "        all_nouns += nouns\n",
    "            \n",
    "    all_nouns = Counter(all_nouns) \n",
    "    all_nouns = {k: v for k, v in sorted(all_nouns.items(), key=lambda item: item[1])}\n",
    "    i = 0\n",
    "    top_20 = [] \n",
    "    for key in all_nouns.keys():\n",
    "        if(i<20):\n",
    "            top_20.append(key)\n",
    "            i+=1\n",
    "        else:\n",
    "            break\n",
    "    print(top_20)\n",
    "    tags.append(top_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_csv('tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
