{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-110-ebc4251703b0>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-110-ebc4251703b0>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    from nltk.tokenize import RegexpTokenizer>\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer>\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "nltk.download('words')\n",
    "from gensim.summarization.summarizer import summarize \n",
    "from gensim.summarization import keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into pandas frame\n",
    "data_frame = pd.DataFrame()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for i in range(1,4,1):\n",
    "    try:\n",
    "        path = './data/articles'+str(i)+'.csv'\n",
    "        if os.path.exists(path):\n",
    "            chunk_list = []\n",
    "            reader_obj = pd.read_csv(path,chunksize=10000) \n",
    "            for chunk in reader_obj:\n",
    "                chunk_list.append(chunk)    \n",
    "            data_frame = pd.concat([data_frame,pd.concat(chunk_list).drop(['Unnamed: 0'],axis=1)], ignore_index=True)\n",
    "    except:\n",
    "        # handle the file not found error\n",
    "        print(sys.exc_info())\n",
    "    \n",
    "print(data_frame.columns)\n",
    "print(data_frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a simple text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5607\n"
     ]
    }
   ],
   "source": [
    "# Displaying a sample content\n",
    "sample_content = data_frame['content'][0]\n",
    "print(len(sample_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "### Tokenizations is the process of separating each and every small letter of the sentence.\n",
    "\n",
    "# Removal of Stop Words: \n",
    "### In this process we are also eliminating the stop words in order to extract only words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880\n"
     ]
    }
   ],
   "source": [
    "def tokenization(content):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    token_list = tokenizer.tokenize(content.lower())\n",
    "    return token_list\n",
    "token_list = tokenization(sample_content)\n",
    "print(len(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "def stop_words_filter(token_list):\n",
    "    stopword_set = set(stopwords.words('english'))    \n",
    "    filtered_tokens = []\n",
    "    for token in token_list:\n",
    "        if (token not in stopword_set \n",
    "        and token.isnumeric() == False \n",
    "        and wordnet.synsets(token) != [] \n",
    "        and token.isalpha()\n",
    "        and len(token)>1):\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    return filtered_tokens\n",
    "\n",
    "print(len(token_list))\n",
    "filtered_tokens = stop_words_filter(token_list)\n",
    "print(len(filtered_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see we were able to remove a lot of unnecessary words from the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "#### For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "#### The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
    "\n",
    "#### &emsp;  &emsp; am, are, is $\\Rightarrow$ be\n",
    "#### &emsp;  &emsp; car, cars, car's, cars' $\\Rightarrow$ car\n",
    "#### The result of this mapping of text will be something like:\n",
    "#### &emsp;  &emsp; the boy's cars are different colors $\\Rightarrow$\n",
    "#### &emsp;  &emsp; the boy car be differ color\n",
    "#### However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.\n",
    "\n",
    "#### For more information refer: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: In the below method if you uncomment the two print lines you will be able to see what this function is doing. This will help to improve the performance of count-based clustering techniques. Also it will reduce the size of sparse matrix.\n",
    "\n",
    "#### Example there are many examples which are being lemmatized like \n",
    "##### eg 1 . rounds => round \n",
    "##### eg 2 . leases => lease\n",
    "##### eg 3 . jobs => job\n",
    "##### eg 4 . appointees => appointee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization \n",
    "def lemmatize_tokens(filtered_tokens):\n",
    "    lemmatized = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token in filtered_tokens:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token))\n",
    "        #print(\"token: \" + token)\n",
    "        #print(\"Lemmantized \"+lemmatizer.lemmatize(token))\n",
    "    return lemmatized\n",
    "lemmatized = lemmatize_tokens(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(data):\n",
    "    summ = summarize(data, ratio = 0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', 'congressional', 'republican', 'new', 'fear', 'health', 'care', 'lawsuit', 'administration', 'trump', 'administration', 'branch', 'suit', 'challenge', 'administration', 'authority', 'dollar', 'health', 'insurance', 'subsidy', 'american', 'handing', 'house', 'republican', 'victory', 'issue', 'loss', 'subsidy', 'health', 'care', 'program', 'implode', 'people', 'health', 'insurance', 'republican', 'replacement', 'lead', 'chaos', 'insurance', 'market', 'backlash', 'republican', 'control', 'government', 'stave', 'outcome', 'republican', 'position', 'sum', 'health', 'care', 'law', 'voter', 'end', 'law', 'year', 'twist', 'trump', 'administration', 'branch', 'prerogative', 'choose', 'republican', 'ally', 'house', 'central', 'question', 'dispute', 'eager', 'avoid', 'pileup', 'republican', 'capitol', 'hill', 'trump', 'transition', 'team', 'lawsuit', 'election', 'put', 'limbo', 'february', 'united', 'state', 'court', 'appeal', 'district', 'columbia', 'circuit', 'divulge', 'strategy', 'litigation', 'administration', 'congress', 'inappropriate', 'comment', 'spokesman', 'trump', 'transition', 'effort', 'office', 'trump', 'administration', 'case', 'aspect', 'care', 'act', 'decision', 'judge', 'rosemary', 'house', 'republican', 'standing', 'sue', 'branch', 'spending', 'dispute', 'administration', 'health', 'insurance', 'subsidy', 'violation', 'constitution', 'approval', 'congress', 'justice', 'department', 'judge', 'decision', 'subsidy', 'place', 'appeal', 'halt', 'mr', 'trump', 'house', 'republican', 'month', 'court', 'transition', 'team', 'option', 'resolution', 'matter', 'effect', 'inauguration', 'jan', 'suspension', 'case', 'house', 'lawyer', 'administration', 'time', 'appeal', 'leadership', 'official', 'house', 'acknowledge', 'possibility', 'effect', 'payment', 'insurer', 'subsidy', 'exchange', 'cost', 'consumer', 'race', 'drop', 'coverage', 'money', 'loss', 'subsidy', 'destabilize', 'program', 'cause', 'lack', 'confidence', 'lead', 'insurer', 'exit', 'administration', 'mount', 'fight', 'house', 'republican', 'dim', 'view', 'health', 'care', 'law', 'team', 'lawyer', 'month', 'case', 'behalf', 'health', 'care', 'program', 'request', 'lawyer', 'deal', 'house', 'republican', 'new', 'administration', 'dismiss', 'settle', 'case', 'produce', 'consequence', 'individual', 'reduction', 'well', 'nation', 'health', 'insurance', 'health', 'care', 'system', 'house', 'republican', 'concept', 'power', 'purse', 'right', 'congress', 'sue', 'branch', 'constitution', 'power', 'house', 'republican', 'congress', 'money', 'subsidy', 'constitution', 'suit', 'john', 'house', 'speaker', 'time', 'house', 'committee', 'report', 'republican', 'asserted', 'administration', 'funding', 'treasury', 'department', 'skepticism', 'white', 'house', 'part', 'law', 'appropriation', 'administration', 'house', 'judge', 'congress', 'sue', 'white', 'house', 'issue', 'expert', 'precedent', 'leverage', 'branch', 'spending', 'power', 'trump', 'administration', 'pressure', 'presidential', 'authority', 'fight', 'house', 'matter', 'view', 'health', 'care', 'precedent', 'repercussion', 'victory', 'house', 'trump', 'era', 'republican', 'house']\n"
     ]
    }
   ],
   "source": [
    "def find_nouns(content,isTokens=True):\n",
    "    if isTokens:\n",
    "        doc = nlp(' '.join(content))\n",
    "    else:\n",
    "        doc = nlp(content)\n",
    "    tokens = []\n",
    "    for index in range(len(doc)):\n",
    "        if doc[index].pos_ == 'NOUN' or doc[index].pos_ == 'PROPN':\n",
    "            tokens.append(doc[index].text)\n",
    "    return tokens\n",
    "nouns = find_nouns(lemmatized)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2851\n",
      "5702\n",
      "8553\n",
      "11404\n",
      "14255\n",
      "17106\n",
      "19957\n",
      "22808\n",
      "25659\n",
      "28510\n",
      "31361\n",
      "34212\n",
      "37063\n",
      "39914\n",
      "42765\n",
      "45616\n",
      "48467\n",
      "51318\n",
      "54169\n",
      "57020\n",
      "59871\n",
      "62722\n",
      "65573\n",
      "68424\n",
      "71275\n",
      "74126\n",
      "76977\n",
      "79828\n",
      "82679\n",
      "85530\n",
      "88381\n",
      "91232\n",
      "94083\n",
      "96934\n",
      "99785\n",
      "102636\n",
      "105487\n",
      "108338\n",
      "111189\n",
      "114040\n",
      "116891\n",
      "119742\n",
      "122593\n",
      "125444\n",
      "128295\n",
      "131146\n",
      "133997\n",
      "136848\n",
      "139699\n",
      "142550\n"
     ]
    }
   ],
   "source": [
    "# Create a data pipeline to process the dataset \n",
    "\n",
    "def data_preprocessing(data_frame):\n",
    "\n",
    "    res = pd.DataFrame(columns=['id','content'])\n",
    "    '''\n",
    "\n",
    "    # Step 1: Tokenization\n",
    "    tmp_data_frame = data_frame['content'][0:1000].apply(lambda row:tokenization(row))\n",
    "\n",
    "    # Step 2: Remove stop words\n",
    "    tmp_data_frame = tmp_data_frame.apply(lambda row: stop_words_filter(row))\n",
    "\n",
    "    # Step 3: Make a string\n",
    "    tmp_data_frame = tmp_data_frame.apply(lambda row: ' '.join(row))\n",
    "\n",
    "    res = pd.concat([res,tmp_data_frame])\n",
    "\n",
    "    '''\n",
    "    total_records = len(data_frame)  #50000\n",
    "    start = 0 \n",
    "    interval = ((total_records - start) // 50 ) #1000\n",
    "    # The processing in chunks will reduce the memory load\n",
    "    for i in range(start,total_records,interval):\n",
    "        print(i)\n",
    "        # Step 1: Shorten the text\n",
    "        if(i+interval < total_records):\n",
    "            tmp_data_frame = data_frame['content'][i:i+interval].apply(lambda row:row[:150])\n",
    "        else:\n",
    "            tmp_data_frame = data_frame['content'][i:total_records].apply(lambda row:row[:150])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Step 3: Tokenize the text\n",
    "        tmp_data_frame = tmp_data_frame.apply(lambda row: tokenization(row))\n",
    "\n",
    "        # Step 2: find Nouns and Pronouns\n",
    "        tmp_data_frame = tmp_data_frame.apply(lambda row: find_nouns(row))\n",
    "        \n",
    "        # Step 4: Remove stop words\n",
    "        #tmp_data_frame = tmp_data_frame.apply(lambda row: stop_words_filter(row))\n",
    "        \n",
    "        # Step 5: Lemmitize tokens\n",
    "        tmp_data_frame = tmp_data_frame.apply(lambda row: lemmatize_tokens(row))\n",
    "                \n",
    "        # Step 6: Make a string\n",
    "        tmp_data_frame = tmp_data_frame.apply(lambda row: ' '.join(row))\n",
    "\n",
    "        #print(tmp_data_frame.to_frame())\n",
    "        res = pd.concat([res,tmp_data_frame.to_frame()])\n",
    "        \n",
    "    res.columns = ['doc_id','content']\n",
    "    res['doc_id'] = range(0,total_records)\n",
    "    return res\n",
    "\n",
    "df = data_preprocessing(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('intermediate_dataframe.csv', sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('intermediate_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "vectorizer = TfidfVectorizer(max_df=0.3,use_idf=True,norm=u'l2',smooth_idf=True)\n",
    "sparse_mat = vectorizer.fit_transform(df['content'].apply(lambda x: np.str_(x)))\n",
    "word_features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 795)\t0.29524144634345095\n",
      "  (0, 33447)\t0.2579470846262801\n",
      "  (0, 26879)\t0.36643473062872195\n",
      "  (0, 7662)\t0.3381882193213997\n",
      "  (0, 21034)\t0.3145692964304565\n",
      "  (0, 16649)\t0.40815749670495943\n",
      "  (0, 39375)\t0.2600954251262091\n",
      "  (0, 10215)\t0.4544612175919412\n",
      "  (0, 50532)\t0.2406638600936217\n",
      "  (1, 41375)\t0.2783925747555006\n",
      "  (1, 11157)\t0.26472030145377734\n",
      "  (1, 51221)\t0.31473857134005395\n",
      "  (1, 35382)\t0.1719065320470829\n",
      "  (1, 7494)\t0.40583357591310804\n",
      "  (1, 14038)\t0.4670134321803686\n",
      "  (1, 5614)\t0.3093371449845102\n",
      "  (1, 42528)\t0.3589471810431394\n",
      "  (1, 6901)\t0.3412821809709657\n",
      "  (2, 6792)\t0.32413779350858446\n",
      "  (2, 45314)\t0.3062683847603152\n",
      "  (2, 11198)\t0.27073853811181975\n",
      "  (2, 3989)\t0.48477864275470517\n",
      "  (2, 13314)\t0.6060585081691526\n",
      "  (2, 50414)\t0.3542848620834026\n",
      "  (3, 10305)\t0.5065997764420475\n",
      "  :\t:\n",
      "  (142566, 35382)\t0.21586123696319226\n",
      "  (142567, 21847)\t0.4860367389387102\n",
      "  (142567, 19530)\t0.5035346910416959\n",
      "  (142567, 16034)\t0.40986888137994876\n",
      "  (142567, 14589)\t0.3183264382226988\n",
      "  (142567, 37321)\t0.33485403361218946\n",
      "  (142567, 795)\t0.27023476188388146\n",
      "  (142567, 33447)\t0.23609919899774412\n",
      "  (142568, 36151)\t0.5347563736467353\n",
      "  (142568, 39008)\t0.5043102404017749\n",
      "  (142568, 16704)\t0.36611040641874326\n",
      "  (142568, 49295)\t0.29627516058235515\n",
      "  (142568, 30436)\t0.29784405898179594\n",
      "  (142568, 19531)\t0.22035652553712068\n",
      "  (142568, 45043)\t0.21050035937780598\n",
      "  (142568, 26853)\t0.23730258887793593\n",
      "  (142569, 38794)\t0.49484137822046104\n",
      "  (142569, 42722)\t0.3593899025649669\n",
      "  (142569, 9690)\t0.33676766078315373\n",
      "  (142569, 29133)\t0.3569556559260678\n",
      "  (142569, 17135)\t0.32441719595982865\n",
      "  (142569, 39415)\t0.31477969943421846\n",
      "  (142569, 32278)\t0.24949256142646306\n",
      "  (142569, 5760)\t0.29615335164903545\n",
      "  (142569, 51878)\t0.1756558811364484\n"
     ]
    }
   ],
   "source": [
    "print(sparse_mat)\n",
    "word_features = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import TruncatedSVD\n",
    "#svd = TruncatedSVD(n_components=200)\n",
    "#sparse_mat = svd.fit_transform(sparse_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "sse = []\n",
    "davies_score =[]\n",
    "'''\n",
    "list_k = list(range(2, 20))\n",
    "for k in list_k:\n",
    "    kmeans = KMeans(n_clusters=k, random_state = 0).fit(sparse_mat)\n",
    "    l = kmeans.labels_\n",
    "    result = metrics.davies_bouldin_score(sparse_mat, l)\n",
    "    davies_score.append(result)\n",
    "    sse.append(kmeans.inertia_)\n",
    "'''\n",
    "kmeans = MiniBatchKMeans(n_clusters = 20).fit(sparse_mat)\n",
    "#kmeans = KMeans(20,init='k-means++', max_iter=100, n_init=1,random_state=0).fit(sparse_mat)\n",
    "\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = metrics.davies_bouldin_score(sparse_mat.toarray(),labels)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'kmeans.pickle'\n",
    "pickle.dump(kmeans, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0285236948673395\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "kmeans = pickle.load(open(filename, 'rb'))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 795)\t0.29524144634345095\n",
      "  (0, 33447)\t0.2579470846262801\n",
      "  (0, 26879)\t0.36643473062872195\n",
      "  (0, 7662)\t0.3381882193213997\n",
      "  (0, 21034)\t0.3145692964304565\n",
      "  (0, 16649)\t0.40815749670495943\n",
      "  (0, 39375)\t0.2600954251262091\n",
      "  (0, 10215)\t0.4544612175919412\n",
      "  (0, 50532)\t0.2406638600936217\n",
      "  (1, 41375)\t0.2783925747555006\n",
      "  (1, 11157)\t0.26472030145377734\n",
      "  (1, 51221)\t0.31473857134005395\n",
      "  (1, 35382)\t0.1719065320470829\n",
      "  (1, 7494)\t0.40583357591310804\n",
      "  (1, 14038)\t0.4670134321803686\n",
      "  (1, 5614)\t0.3093371449845102\n",
      "  (1, 42528)\t0.3589471810431394\n",
      "  (1, 6901)\t0.3412821809709657\n",
      "  (2, 6792)\t0.32413779350858446\n",
      "  (2, 45314)\t0.3062683847603152\n",
      "  (2, 11198)\t0.27073853811181975\n",
      "  (2, 3989)\t0.48477864275470517\n",
      "  (2, 13314)\t0.6060585081691526\n",
      "  (2, 50414)\t0.3542848620834026\n",
      "  (3, 10305)\t0.5065997764420475\n",
      "  :\t:\n",
      "  (142566, 35382)\t0.21586123696319226\n",
      "  (142567, 21847)\t0.4860367389387102\n",
      "  (142567, 19530)\t0.5035346910416959\n",
      "  (142567, 16034)\t0.40986888137994876\n",
      "  (142567, 14589)\t0.3183264382226988\n",
      "  (142567, 37321)\t0.33485403361218946\n",
      "  (142567, 795)\t0.27023476188388146\n",
      "  (142567, 33447)\t0.23609919899774412\n",
      "  (142568, 36151)\t0.5347563736467353\n",
      "  (142568, 39008)\t0.5043102404017749\n",
      "  (142568, 16704)\t0.36611040641874326\n",
      "  (142568, 49295)\t0.29627516058235515\n",
      "  (142568, 30436)\t0.29784405898179594\n",
      "  (142568, 19531)\t0.22035652553712068\n",
      "  (142568, 45043)\t0.21050035937780598\n",
      "  (142568, 26853)\t0.23730258887793593\n",
      "  (142569, 38794)\t0.49484137822046104\n",
      "  (142569, 42722)\t0.3593899025649669\n",
      "  (142569, 9690)\t0.33676766078315373\n",
      "  (142569, 29133)\t0.3569556559260678\n",
      "  (142569, 17135)\t0.32441719595982865\n",
      "  (142569, 39415)\t0.31477969943421846\n",
      "  (142569, 32278)\t0.24949256142646306\n",
      "  (142569, 5760)\t0.29615335164903545\n",
      "  (142569, 51878)\t0.1756558811364484\n",
      "52522\n"
     ]
    }
   ],
   "source": [
    "print(sparse_mat)\n",
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(6, 6))\\nplt.plot(list_k, sse, '-o')\\nplt.xlabel(r'Number of clusters *k*')\\nplt.ylabel('Sum of squared distance');\\nplt.savefig('results/20ClustersSSE.png')\\n\""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot sse against k\n",
    "'''\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');\n",
    "plt.savefig('results/20ClustersSSE.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(6, 6))\\nplt.plot(list_k, davies_score, '-o')\\nplt.xlabel(r'Number of clusters *k*')\\nplt.ylabel('Davies Scores');\\nplt.savefig('results/DaviesScore_20Clusters.png')\\n\""
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, davies_score, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Davies Scores');\n",
    "plt.savefig('results/DaviesScore_20Clusters.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Optimal Cluster size: \"+ str(list_k[np.argmin(davies_score)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the topic name\n",
    "clusters = []\n",
    "for i in range(0,20,1):\n",
    "    cluster_tuple = np.where(labels == i)\n",
    "    clusters.append(cluster_tuple[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Cluster ID 0:\n",
      "['convicti', 'patz', 'disappearance', 'mystery', 'jury', 'trial', 'manhattan', 'tuesday', 'year', 'foota']\n",
      "Cluster ID 1:\n",
      "['kenya', 'blackout', 'monkey', 'elect', 'knee', 'infrastructure', 'hour', 'tuesday', 'footag', 'foota']\n",
      "Cluster ID 2:\n",
      "['application', 'gawker', 'ability', 'launch', 'concern', 'crime', 'apple', 'time', 'week', 'football']\n",
      "Cluster ID 3:\n",
      "['deba', 'houston', 'season', 'gop', 'night', 'thursday', 'donald', 'trump', 'forbi', 'forbiddin']\n",
      "Cluster ID 4:\n",
      "['change', 'penny', 'mike', 'vice', 'climate', 'president', 'trump', 'indiana', 'gov', 'donald']\n",
      "Cluster ID 5:\n",
      "['year', 'cnn', 'people', 'day', 'time', 'new', 'police', 'city', 'woman', 'tuesday']\n",
      "Cluster ID 6:\n",
      "['ashe', 'semifinal', 'serena', 'arthur', 'stadium', 'williams', 'sister', 'career', 'foodporn', 'fontana']\n",
      "Cluster ID 7:\n",
      "['trump', 'donald', 'president', 'campaign', 'washington', 'republican', 'nominee', 'cnn', 'house', 'white']\n",
      "Cluster ID 8:\n",
      "['breslaw', 'anna', 'alcohol', 'drink', 'commitment', 'writer', 'manhattan', 'foot', 'footcare', 'footballer']\n",
      "Cluster ID 9:\n",
      "['news', 'breitbart', 'fox', 'host', 'channel', 'siriusxm', 'daily', 'trump', 'donald', 'am']\n",
      "Cluster ID 10:\n",
      "['photo', 'hardship', 'siena', 'indication', 'beauty', 'answer', 'international', 'award', 'life', 'month']\n",
      "Cluster ID 11:\n",
      "['spicer', 'press', 'scrutiny', 'sean', 'briefing', 'secretary', 'white', 'house', 'thursday', 'trump']\n",
      "Cluster ID 12:\n",
      "['state', 'united', 'president', 'nation', 'trump', 'cnn', 'donald', 'secretary', 'washington', 'year']\n",
      "Cluster ID 13:\n",
      "['ad', 'extension', 'fol', 'story', 'food', 'foot', 'footcare', 'footballer', 'football', 'footbal']\n",
      "Cluster ID 14:\n",
      "['fedna', 'hermosante', 'chantal', 'creek', 'haiti', 'wind', 'hurricane', 'bank', 'wall', 'home']\n",
      "Cluster ID 15:\n",
      "['week', 'trump', 'cnn', 'president', 'donald', 'time', 'day', 'election', 'house', 'new']\n",
      "Cluster ID 16:\n",
      "['obama', 'president', 'barack', 'administration', 'trump', 'washington', 'house', 'white', 'donald', 'cnn']\n",
      "Cluster ID 17:\n",
      "['softbank', 'telecom', 'conglomerate', 'internet', 'united', 'tuesday', 'state', 'donald', 'trump', 'foodie']\n",
      "Cluster ID 18:\n",
      "['clinton', 'hillary', 'donald', 'trump', 'campaign', 'sander', 'bernie', 'email', 'nominee', 'candidate']\n",
      "Cluster ID 19:\n",
      "['annibale', 'gasparis', 'asteroid', 'italian', 'jupiter', 'astronomer', 'mar', 'march', 'de', 'footbal']\n"
     ]
    }
   ],
   "source": [
    "print(len(clusters[2]))\n",
    "# centers of the clusters\n",
    "centers = kmeans.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "tags = []\n",
    "for i in range(0,20):\n",
    "    word_list=[]\n",
    "    print(\"Cluster ID %d:\"% i)\n",
    "    for j in centers[i,:10]:\n",
    "        word_list.append(terms[j])\n",
    "    print(word_list) \n",
    "    tags.append(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-4299b80296a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tags.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\dharmang solanki\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'to_csv'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
